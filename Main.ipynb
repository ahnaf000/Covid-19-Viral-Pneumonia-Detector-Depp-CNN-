{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2e90076",
   "metadata": {},
   "source": [
    "# COVID + VIRAL PNEUMONIA DETECTING CONVOLUTIONAL NEURAL NETWORK MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "fd868f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.optimizers import RMSprop,Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.utils.np_utils import to_categorical \n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, classification_report\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e3e87b",
   "metadata": {},
   "source": [
    "In this project, I am building a CNN to classify images. Given the X-ray image of a patient, my CNN model will be able to predict whether the person has Covid, Viral Pneumonia, or a normal ( not Covid, or Viral pneumonia) X-ray report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb37d4b",
   "metadata": {},
   "source": [
    "# Image Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626dcf56",
   "metadata": {},
   "source": [
    "I am going to use the following labels to classify the images:\n",
    "\n",
    "0. Normal\n",
    "1. Covid\n",
    "2. Viral Pneumonia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "4d345851",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_image_types = [\".jpeg\",\".jpg\",\".gif\",\".png\",\".tga\"] # these are the image extensions my model initially recognizes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40febc3e",
   "metadata": {},
   "source": [
    "### Converting all training images into .png file type and reshaping them\n",
    "Given a varity of image types, it is imprtant to convert them all under one single hood. Different image types are configured in different ways if we look at it from a lower level. \n",
    "\n",
    "I decided to choose .png as my image type, and given any size of image, the model will resize it to a shape of 450x450 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "f1d9b537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Processing Training Images\n"
     ]
    }
   ],
   "source": [
    "count=0 # Index of images\n",
    "\n",
    "path = \"Data/train/Covid\"\n",
    "for file in os.listdir(path):\n",
    "    img_type =  os.path.splitext(file)[1]\n",
    "    \n",
    "    if img_type in valid_image_types:\n",
    "        im = Image.open(os.path.join(path,file))\n",
    "        im = im.resize((450,450)) # reshaping the image\n",
    "        new_path = 'Data/newData/train2/'+str(count)+'.png'\n",
    "        im.save(new_path)\n",
    "        count+=1\n",
    "        \n",
    "path = \"Data/train/Normal\"\n",
    "for file in os.listdir(path):\n",
    "    img_type =  os.path.splitext(file)[1]\n",
    "    \n",
    "    if img_type in valid_image_types:\n",
    "        im = Image.open(os.path.join(path,file))\n",
    "        im = im.resize((450,450)) # reshaping the image\n",
    "        new_path = 'Data/newData/train2/'+str(count)+'.png'\n",
    "        im.save(new_path)\n",
    "        count+=1\n",
    "        \n",
    "path = \"Data/train/Viral Pneumonia\"\n",
    "for file in os.listdir(path):\n",
    "    img_type =  os.path.splitext(file)[1]\n",
    "    \n",
    "    if img_type in valid_image_types:\n",
    "\n",
    "        im = Image.open(os.path.join(path,file))\n",
    "        im = im.resize((450,450)) # reshaping the image\n",
    "        new_path = 'Data/newData/train2/'+str(count)+'.png'\n",
    "        im.save(new_path)\n",
    "        count+=1\n",
    "        \n",
    "print(\"Done Processing Training Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0702b5",
   "metadata": {},
   "source": [
    "### Converting all testing images into .png file type and reshaping them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "198b5241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Processing Testing Images\n"
     ]
    }
   ],
   "source": [
    "count=0 # Index of images\n",
    "\n",
    "path = \"Data/test/Covid\"\n",
    "for file in os.listdir(path):\n",
    "    img_type =  os.path.splitext(file)[1]\n",
    "    \n",
    "    if img_type in valid_image_types:\n",
    "        im = Image.open(os.path.join(path,file))\n",
    "        im = im.resize((450,450)) # reshaping the image\n",
    "        new_path = 'Data/newData/test2/'+str(count)+'.png'\n",
    "        im.save(new_path)\n",
    "        count+=1\n",
    "        \n",
    "path = \"Data/test/Normal\"\n",
    "for file in os.listdir(path):\n",
    "    img_type =  os.path.splitext(file)[1]\n",
    "    \n",
    "    if img_type in valid_image_types:\n",
    "        im = Image.open(os.path.join(path,file))\n",
    "        im = im.resize((450,450)) # reshaping the image\n",
    "        new_path = 'Data/newData/test2/'+str(count)+'.png'\n",
    "        im.save(new_path)\n",
    "        count+=1\n",
    "        \n",
    "path = \"Data/test/Viral Pneumonia\"\n",
    "for file in os.listdir(path):\n",
    "    img_type =  os.path.splitext(file)[1]\n",
    "    \n",
    "    if img_type in valid_image_types:\n",
    "        im = Image.open(os.path.join(path,file))\n",
    "        im = im.resize((450,450)) # reshaping the image\n",
    "        new_path = 'Data/newData/test2/'+str(count)+'.png'\n",
    "        im.save(new_path)\n",
    "        count+=1\n",
    "        \n",
    "print(\"Done Processing Testing Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14775548",
   "metadata": {},
   "source": [
    "# Cleaning the Training Data\n",
    "Now, it is time to read the newly configured images and turning them into numpy arrays of pixel intensities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "f82f8401",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Data/newData/train2\" # src folder containing the images\n",
    "\n",
    "X_train = np.empty((251,450,450,1))\n",
    "training_data_labels_index = {}\n",
    "for i in range(0,111): training_data_labels_index[i] = 1\n",
    "for i in range(111,181): training_data_labels_index[i] = 0\n",
    "for i in range(181,251): training_data_labels_index[i] = 2\n",
    "    \n",
    "# Loading the images and converting them into pixels\n",
    "for img in os.listdir(path):\n",
    "    ori_img = mpimg.imread(os.path.join(path,img))\n",
    "    # Handling the pixel density\n",
    "    if ori_img.ndim == 2:\n",
    "        image = ori_img[:,:]\n",
    "    elif ori_img.ndim == 3:\n",
    "        image = ori_img[:,:,0]\n",
    "    else:\n",
    "        image = ori_img[:,:,:,0]\n",
    "        \n",
    "    '''mapping the image to it's corresponding label'''\n",
    "    ind = img.index(\".png\")\n",
    "    label = int(img[:ind])\n",
    "    X_train[label,:,:,:] = image.reshape((450,450,1)) # Adding to the training array\n",
    "\n",
    "y_train = np.empty((251,))\n",
    "y_train[0:111] = 1\n",
    "y_train[111:181] = 0\n",
    "y_train[181:251] = 2\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Label Encoding \n",
    "y_train = to_categorical(y_train, num_classes = 3)\n",
    "y_val = to_categorical(y_val, num_classes = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8310a6",
   "metadata": {},
   "source": [
    "# Cleaning the Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "0df27144",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Data/newData/test2\" # src folder containing the images\n",
    "\n",
    "X_test = np.empty((66,450,450,1))\n",
    "\n",
    "testing_data_labels_index = {}\n",
    "for i in range(0,26): testing_data_labels_index[i] = 1\n",
    "for i in range(26,46): testing_data_labels_index[i] = 0\n",
    "for i in range(46,66): testing_data_labels_index[i] = 2\n",
    "\n",
    "# Loading the images and converting them into pixels\n",
    "for img in os.listdir(path):\n",
    "    ori_img = mpimg.imread(os.path.join(path,img))\n",
    "    # Handling the pixel density\n",
    "    if ori_img.ndim == 2:\n",
    "        image = ori_img[:,:]\n",
    "    elif ori_img.ndim == 3:\n",
    "        image = ori_img[:,:,0]\n",
    "    else:\n",
    "        image = ori_img[:,:,:,0]\n",
    "        \n",
    "    '''mapping the image to it's corresponding label'''\n",
    "    ind = img.index(\".png\")\n",
    "    label = int(img[:ind])\n",
    "    X_test[label,:,:,:] = image.reshape((450,450,1)) # Adding to the training array\n",
    "    \n",
    "y_test = np.empty((66,))\n",
    "y_test[0:26] = 1\n",
    "y_test[26:46] = 0\n",
    "y_test[46:66] = 2\n",
    "# Label Encoding \n",
    "y_test = to_categorical(y_test, num_classes = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bdb789",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "f543b792",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # dimesion reduction\n",
    "        rotation_range=5,  # randomly rotate images in the range 5 degrees\n",
    "        zoom_range = 0.1, # Randomly zoom image 10%\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally 10%\n",
    "        height_shift_range=0.1,  # randomly shift images vertically 10%\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a9123f",
   "metadata": {},
   "source": [
    "# Building the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "60bf2518",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 8, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu', input_shape = (450,450,1)))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Conv2D(filters = 16, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(900, activation = \"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation = \"softmax\"))\n",
    "\n",
    "# Defining the optimizer\n",
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "optimizer=Adam()\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Model Hyperparameters\n",
    "epochs = 50\n",
    "batch_size = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45507f4f",
   "metadata": {},
   "source": [
    "## Training the CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "67c7e6bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "15/15 [==============================] - 42s 3s/step - loss: 65.2940 - accuracy: 0.3473 - val_loss: 5.1388 - val_accuracy: 0.6078\n",
      "Epoch 2/50\n",
      "15/15 [==============================] - 37s 2s/step - loss: 8.9566 - accuracy: 0.5918 - val_loss: 1.6686 - val_accuracy: 0.6471\n",
      "Epoch 3/50\n",
      "15/15 [==============================] - 37s 2s/step - loss: 2.9363 - accuracy: 0.7698 - val_loss: 0.4307 - val_accuracy: 0.9020\n",
      "Epoch 4/50\n",
      "15/15 [==============================] - 37s 2s/step - loss: 0.8535 - accuracy: 0.8578 - val_loss: 0.6053 - val_accuracy: 0.9020\n",
      "Epoch 5/50\n",
      "15/15 [==============================] - 36s 2s/step - loss: 0.5252 - accuracy: 0.8826 - val_loss: 1.0102 - val_accuracy: 0.7255\n",
      "Epoch 6/50\n",
      "15/15 [==============================] - 38s 3s/step - loss: 0.3078 - accuracy: 0.8769 - val_loss: 0.9151 - val_accuracy: 0.8039\n",
      "Epoch 7/50\n",
      "15/15 [==============================] - 39s 3s/step - loss: 0.1166 - accuracy: 0.9502 - val_loss: 0.7802 - val_accuracy: 0.7647\n",
      "Epoch 8/50\n",
      "15/15 [==============================] - 37s 2s/step - loss: 0.1047 - accuracy: 0.9420 - val_loss: 0.7025 - val_accuracy: 0.8039\n",
      "Epoch 9/50\n",
      "15/15 [==============================] - 37s 3s/step - loss: 0.0767 - accuracy: 0.9746 - val_loss: 0.7373 - val_accuracy: 0.7647\n",
      "Epoch 10/50\n",
      "15/15 [==============================] - 36s 2s/step - loss: 0.0624 - accuracy: 0.9856 - val_loss: 0.6640 - val_accuracy: 0.7647\n",
      "Epoch 11/50\n",
      "15/15 [==============================] - 37s 2s/step - loss: 0.0393 - accuracy: 0.9885 - val_loss: 0.7059 - val_accuracy: 0.7451\n",
      "Epoch 12/50\n",
      "15/15 [==============================] - 36s 2s/step - loss: 0.0399 - accuracy: 0.9893 - val_loss: 0.6577 - val_accuracy: 0.7255\n",
      "Epoch 13/50\n",
      "15/15 [==============================] - 37s 2s/step - loss: 0.0418 - accuracy: 0.9836 - val_loss: 0.6569 - val_accuracy: 0.8235\n",
      "Epoch 14/50\n",
      "15/15 [==============================] - 36s 2s/step - loss: 0.0143 - accuracy: 1.0000 - val_loss: 0.6049 - val_accuracy: 0.8431\n",
      "Epoch 15/50\n",
      "15/15 [==============================] - 37s 2s/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.5394 - val_accuracy: 0.8627\n",
      "Epoch 16/50\n",
      "15/15 [==============================] - 37s 2s/step - loss: 0.0059 - accuracy: 0.9983 - val_loss: 0.5014 - val_accuracy: 0.8627\n",
      "Epoch 17/50\n",
      "15/15 [==============================] - 38s 3s/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.4679 - val_accuracy: 0.8627\n",
      "Epoch 18/50\n",
      "15/15 [==============================] - 38s 3s/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.4052 - val_accuracy: 0.8824\n",
      "Epoch 19/50\n",
      "15/15 [==============================] - 37s 2s/step - loss: 0.0171 - accuracy: 0.9916 - val_loss: 0.4902 - val_accuracy: 0.8235\n",
      "Epoch 20/50\n",
      "15/15 [==============================] - 37s 2s/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.4740 - val_accuracy: 0.8431\n",
      "Epoch 21/50\n",
      "15/15 [==============================] - 37s 2s/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.4139 - val_accuracy: 0.8824\n",
      "Epoch 22/50\n",
      "15/15 [==============================] - 37s 2s/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.4006 - val_accuracy: 0.8627\n",
      "Epoch 23/50\n",
      "15/15 [==============================] - 37s 2s/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.3678 - val_accuracy: 0.8824\n",
      "Epoch 24/50\n",
      "15/15 [==============================] - 37s 2s/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.3487 - val_accuracy: 0.8824\n",
      "Epoch 25/50\n",
      "15/15 [==============================] - 39s 3s/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.3113 - val_accuracy: 0.9020\n",
      "Epoch 26/50\n",
      "15/15 [==============================] - 40s 3s/step - loss: 7.6932e-04 - accuracy: 1.0000 - val_loss: 0.2931 - val_accuracy: 0.9020\n",
      "Epoch 27/50\n",
      "15/15 [==============================] - 37s 2s/step - loss: 5.6060e-04 - accuracy: 1.0000 - val_loss: 0.2835 - val_accuracy: 0.9020\n",
      "Epoch 28/50\n",
      "15/15 [==============================] - 37s 2s/step - loss: 9.8604e-04 - accuracy: 1.0000 - val_loss: 0.2825 - val_accuracy: 0.9020\n",
      "Epoch 29/50\n",
      "15/15 [==============================] - 37s 2s/step - loss: 4.9092e-04 - accuracy: 1.0000 - val_loss: 0.2740 - val_accuracy: 0.9020\n",
      "Epoch 30/50\n",
      "15/15 [==============================] - 37s 2s/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.2489 - val_accuracy: 0.9020\n",
      "Epoch 31/50\n",
      "15/15 [==============================] - 37s 2s/step - loss: 8.8031e-04 - accuracy: 1.0000 - val_loss: 0.2379 - val_accuracy: 0.9020\n",
      "Epoch 32/50\n",
      "15/15 [==============================] - 37s 2s/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.2578 - val_accuracy: 0.8824\n",
      "Epoch 33/50\n",
      "15/15 [==============================] - 37s 2s/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.2654 - val_accuracy: 0.8824\n",
      "Epoch 34/50\n",
      "15/15 [==============================] - 37s 2s/step - loss: 7.5599e-04 - accuracy: 1.0000 - val_loss: 0.2670 - val_accuracy: 0.9020\n",
      "Epoch 35/50\n",
      "15/15 [==============================] - 36s 2s/step - loss: 4.8570e-04 - accuracy: 1.0000 - val_loss: 0.2504 - val_accuracy: 0.9020\n",
      "Epoch 36/50\n",
      "15/15 [==============================] - 37s 2s/step - loss: 7.9595e-04 - accuracy: 1.0000 - val_loss: 0.2305 - val_accuracy: 0.9412\n",
      "Epoch 37/50\n",
      "15/15 [==============================] - 37s 2s/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2340 - val_accuracy: 0.9216\n",
      "Epoch 38/50\n",
      "15/15 [==============================] - 38s 3s/step - loss: 1.7248e-04 - accuracy: 1.0000 - val_loss: 0.2290 - val_accuracy: 0.9020\n",
      "Epoch 39/50\n",
      "15/15 [==============================] - 38s 3s/step - loss: 1.9183e-04 - accuracy: 1.0000 - val_loss: 0.2272 - val_accuracy: 0.9020\n",
      "Epoch 40/50\n",
      "15/15 [==============================] - 39s 3s/step - loss: 4.3527e-04 - accuracy: 1.0000 - val_loss: 0.2146 - val_accuracy: 0.9020\n",
      "Epoch 41/50\n",
      "15/15 [==============================] - 37s 2s/step - loss: 3.8462e-04 - accuracy: 1.0000 - val_loss: 0.2183 - val_accuracy: 0.9216\n",
      "Epoch 42/50\n",
      "15/15 [==============================] - 37s 2s/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.2981 - val_accuracy: 0.8824\n",
      "Epoch 43/50\n",
      "15/15 [==============================] - 38s 3s/step - loss: 6.3319e-04 - accuracy: 1.0000 - val_loss: 0.2830 - val_accuracy: 0.8824\n",
      "Epoch 44/50\n",
      "15/15 [==============================] - 37s 2s/step - loss: 7.4554e-04 - accuracy: 1.0000 - val_loss: 0.2618 - val_accuracy: 0.8824\n",
      "Epoch 45/50\n",
      "15/15 [==============================] - 37s 2s/step - loss: 6.5771e-04 - accuracy: 1.0000 - val_loss: 0.2441 - val_accuracy: 0.8824\n",
      "Epoch 46/50\n",
      "15/15 [==============================] - 37s 2s/step - loss: 3.0204e-04 - accuracy: 1.0000 - val_loss: 0.2366 - val_accuracy: 0.8824\n",
      "Epoch 47/50\n",
      "15/15 [==============================] - 37s 2s/step - loss: 4.0024e-04 - accuracy: 1.0000 - val_loss: 0.2349 - val_accuracy: 0.8824\n",
      "Epoch 48/50\n",
      "15/15 [==============================] - 38s 3s/step - loss: 3.2574e-04 - accuracy: 1.0000 - val_loss: 0.2313 - val_accuracy: 0.9020\n",
      "Epoch 49/50\n",
      "15/15 [==============================] - 38s 3s/step - loss: 1.4214e-04 - accuracy: 1.0000 - val_loss: 0.2287 - val_accuracy: 0.9216\n",
      "Epoch 50/50\n",
      "15/15 [==============================] - 38s 3s/step - loss: 2.5988e-04 - accuracy: 1.0000 - val_loss: 0.2274 - val_accuracy: 0.9216\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527d0fa1",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "64537307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict the values from the Testing dataset\n",
    "y_hat = model.predict(X_test)\n",
    "# Convert predictions classes to one hot vectors \n",
    "y_hat_classes = np.argmax(y_hat,axis = 1) \n",
    "# Convert testing observations to one hot vectors\n",
    "y_test = np.argmax(y_test,axis = 1) \n",
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(y_test, y_hat_classes) \n",
    "# plot the confusion matrix\n",
    "f,ax = plt.subplots(figsize=(8, 8))\n",
    "sns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"Greens\",linecolor=\"gray\", fmt= '.1f',ax=ax)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "39e3d8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the CNN: 92.4%\n"
     ]
    }
   ],
   "source": [
    "y_hat_classes = y_hat_classes.reshape((-1,1))\n",
    "y_test = y_test.reshape((-1,1))\n",
    "\n",
    "acc_scr = (round(accuracy_score(y_test, y_hat_classes), 3) * 100)\n",
    "print(f\"Accuracy of the CNN model: {acc_scr}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "50deb3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95        20\n",
      "           1       0.90      1.00      0.95        26\n",
      "           2       0.94      0.80      0.86        20\n",
      "\n",
      "    accuracy                           0.92        66\n",
      "   macro avg       0.93      0.92      0.92        66\n",
      "weighted avg       0.93      0.92      0.92        66\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_hat_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f6c112",
   "metadata": {},
   "source": [
    "# Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "6a2726e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_22 (Conv2D)           (None, 450, 450, 8)       208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 225, 225, 8)       0         \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 225, 225, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 225, 225, 16)      1168      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 112, 112, 16)      0         \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 112, 112, 16)      0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 200704)            0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 900)               180634500 \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 900)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 3)                 2703      \n",
      "=================================================================\n",
      "Total params: 180,638,579\n",
      "Trainable params: 180,638,579\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504fb1f4",
   "metadata": {},
   "source": [
    "# Saving the model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "b023c66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Final Model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('Final Model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba06fdf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
